{
  "automated-chart-generation": {
    "layout": "../_tech_post",
    "date": "December 18, 2009",
    "title": "Automated Chart Generation",
    "excerpt": "It’s late on the Friday afternoon before Christmas week which means things are pretty quiet around the office. This quiet has the net-effect of allowing me to get quite a bit done. The last few days have been very productive with respect to our research project and Azure work (more on that coming soon) which is now in full swing. We are currently working on collecting performance data from our codes running in Azure (and soon in the Amazon cloud) and are also doing some testing of transfer speeds of data both to/from the cloud as well as between compute and storage in the cloud.",
    "categories": ["Miscellaneous"],
    "tags": ["charts", "tools", "vs2010"]
  },
  "windows-azure-climate-data-and-microsoft-surface": {
    "layout": "../_tech_post",
    "date": "December 18, 2009",
    "title": "Windows Azure, Climate Data, and Microsoft Surface",
    "excerpt": "I’ve been working on moving a large collection data to, from, and around Azure as we are testing the data profile for scientific computing and large-scale experiment post-processing and, in order to verify the data we uploaded and processed turned out as we wanted tit to, I built a simple visualization app that does a real-time query against the data in Azure and displays it. ",
    "categories": ["Cloud Computing"],
    "tags": ["azure", "climate", "cloud", "ornl", "samples", "surface"]
  },
  "huntug-meeting-090914": {
    "layout": "../_tech_post",
    "date": "September 14, 2009",
    "title": "HUNTUG Meeting 090914",
    "excerpt": "I’ve been working for a bit on some larger-scale jobs targeting the Windows Azure platform and early last week had assembled a collection of worker roles that were supposed to be processing my datasets for a number of days moving forward. Unfortunately, they wouldn’t stay running. As always, they “worked on my machine”, so I naturally assumed that the problem was with the Azure platform :). I then proceeded to do what I thought was the correct action… go to the Azure portal and request that the logs be transferred to my storage account so I could review them and fix the problem.",
    "categories": ["Cloud Computing", "Conferences"],
    "tags": ["azure", "huntug", "cloud", "ineta", "slides"]
  },
  "live-monitoring-of-your-worker-roles-in-azure": {
    "layout": "../_tech_post",
    "date": "September 3, 2009",
    "title": "“Live” Monitoring Of Your Worker Roles in Azure",
    "excerpt": "I’ve been working for a bit on some larger-scale jobs targeting the Windows Azure platform and early last week had assembled a collection of worker roles that were supposed to be processing my datasets for a number of days moving forward. Unfortunately, they wouldn’t stay running. As always, they “worked on my machine”, so I naturally assumed that the problem was with the Azure platform :). I then proceeded to do what I thought was the correct action… go to the Azure portal and request that the logs be transferred to my storage account so I could review them and fix the problem.",
    "categories": ["Cloud Computing"],
    "tags": ["azure", ".net", "cloud"]
  },
  "silverlight-and-paging-with-azure-data": {
    "layout": "../_tech_post",
    "date": "August 20, 2009",
    "title": "SilverLight and Paging with Azure Data",
    "excerpt": "If you’ve been watching by blog at all lately, you know that I’ve been playing with some larger data sets and Azure storage, specifically Azure table storage. Last week I found myself working with a SilverLight application to visualize the resulting data and display it to the user, however I did not want to use the ADO.NET Data Services client (ATOM) due to the size of data in transmission. Consequently, I set up a web role that proxied the data calls and fed them back to the caller as JSON.",
    "categories": ["Cloud Computing"],
    "tags": ["azure", "silverlight"]
  },
  "atompub-json-azure-and-large-datasets-part-2": {
    "layout": "../_tech_post",
    "date": "August 20, 2009",
    "title": "AtomPub, JSON, Azure and Large Datasets, Part 2",
    "excerpt": "Last Friday I posted some initial results from some simplistic testing I had done comparing pulling data from Azure via ATOM (the ADO.NET data services client) and JSON. I was surprised at the significant difference in payload and time to completion. A little later, Steve Marx questioned my methodology based on the fact that Azure storage doesn’t support JSON. Steve wasn’t being contrary, but rather pushing for clarification to the methodology of my testing as well as a desire to keep people from attempting to exploit the JSON interface of Azure storage when none exists.",
    "categories": ["Cloud Computing"],
    "tags": ["azure", "cloud", "storage"]
  },
  "atompub-json-azure-and-large-datasets": {
    "layout": "../_tech_post",
    "date": "August 14, 2009",
    "title": "AtomPub, JSON, Azure and Large Datasets",
    "excerpt": "I’m just really beginning to scratch the surface on my work on cloud computing and scientific computing but it seems that nearly every day I’m able to spend time on this I come away with something at least moderately novel. Today’s observation is, on reflection, a bit of a no-brainer but it wasn’t immediately obvious to me.",
    "categories": ["Cloud Computing"],
    "tags": ["azure", "cloud", "storage"]
  },
  "azure-visualization-and-large-datasets": {
    "layout": "../_tech_post",
    "date": "August 5, 2009",
    "title": "Azure, Visualization, and Large Datasets",
    "excerpt": "I’ve been working on kicking the tires of Azure’s data functions and in the process was able to get my hands on a large set of climate data for testing purposes. I’m fighting with some size issues and azure, but thought I’d start by loading up some experimental temperature runs into Azure tables and then build a visualization tool to help the viewer to wrap his/her mind around the numbers. This is my first real Silverlight app and, while it has a long way to go, it’s an interesting first stab.",
    "categories": ["Cloud Computing"],
    "tags": ["azure"]
  },
  "silverlight-and-azure-table-data-paging": {
    "layout": "../_tech_post",
    "date": "August 5, 2009",
    "title": "Silverlight and Azure Table Data Paging",
    "excerpt": "I’m playing around with a data visualization app using Silverlight and data hosted in Azure Tables and have been learning quite a bit in the process. Firstly, Azure tables only allows you to return 1000 records in a given query. If you issue a query that has a larger matching result set, Azure will return some extra headers indicating as such (x-ms-continuation-NextPartitionKey and x-ms-continuation-NextRowKey).",
    "categories": ["Cloud Computing"],
    "tags": ["azure", "cloud", "silverlight"]
  },
  "azure-blob-storage-blob-ids-and": {
    "layout": "../_tech_post",
    "date": "July 30, 2009",
    "title": "Azure Blob Storage Blob IDs and “+”",
    "excerpt": "I’d like to thank all of you who attended my session today at CodeStock. I had a great time talking with you all and sharing my experiences with SharePoint and TFS with you all.",
    "categories": ["Cloud Computing", "General Development"],
    "tags": ["azure", "cloud"]
  },
  "codestock-session": {
    "layout": "../_tech_post",
    "date": "June 26, 2009",
    "title": "Codestock Session",
    "excerpt": "I’d like to thank all of you who attended my session today at CodeStock. I had a great time talking with you all and sharing my experiences with SharePoint and TFS with you all.",
    "categories": ["Conferences", "SharePoint"],
    "tags": [".net", "build", "codestock", "sharepoint", "tfs"]
  },
  "customizations-to-stsdev-13": {
    "layout": "../_tech_post",
    "date": "June 23, 2009",
    "title": "Customizations to STSDev 1.3",
    "excerpt": "I’m working on a project which has as one of its goals the “publishing” of some very large datasets (order of 1PB) to the “cloud” for consumption by the general populous for use in scientific research. Rather than designing/inventing our own API, our decision has been to provide an interface consistent with the APIs produced by some of the leading cloud storage providers. Our goal would be that if someone is already used to/has tooling to working with cloud data sources such as Amazon’s S3 service or Microsoft’s Azure Blob storage",
    "categories": ["Conferences", "SharePoint"],
    "tags": [".net", "build", "codestock", "sharepoint", "tfs"]
  },
  "speaking-at-codestock": {
    "layout": "../_tech_post",
    "date": "June 22, 2009",
    "title": "Speaking at CodeStock",
    "excerpt": "I’m working on a project which has as one of its goals the “publishing” of some very large datasets (order of 1PB) to the “cloud” for consumption by the general populous for use in scientific research. Rather than designing/inventing our own API, our decision has been to provide an interface consistent with the APIs produced by some of the leading cloud storage providers. Our goal would be that if someone is already used to/has tooling to working with cloud data sources such as Amazon’s S3 service or Microsoft’s Azure Blob storage",
    "categories": ["Conferences"],
    "tags": [".net", "build", "codestock", "moss", "teamsystem", "tfs"]
  },
  "common-api-set-for-cloud-storage": {
    "layout": "../_tech_post",
    "date": "April 9, 2009",
    "title": "Common API Set for Cloud Storage?",
    "excerpt": "I’m working on a project which has as one of its goals the “publishing” of some very large datasets (order of 1PB) to the “cloud” for consumption by the general populous for use in scientific research. Rather than designing/inventing our own API, our decision has been to provide an interface consistent with the APIs produced by some of the leading cloud storage providers. Our goal would be that if someone is already used to/has tooling to working with cloud data sources such as Amazon’s S3 service or Microsoft’s Azure Blob storage",
    "categories": ["Cloud Computing"],
    "tags": ["aws", "azure"]
  },
  "clouds-and-traditional-hosting-companies": {
    "layout": "../_tech_post",
    "date": "March 25, 2009",
    "title": "Clouds and Traditional Hosting Companies",
    "excerpt": "I sat on a conference call yesterday wherein we discussed our thoughts on where the cloud was going and how it was going to impact the “traditional” hoster. Our company has been working with hosters for many  years (I started in 2000 and they had been doing it before that) and have seen cycles of service come and go. It used to be that just providing personal web + email was all you had to do to be successful. Then, the “Application Service Provider” (ASP) model started and morphed into providing business-related services and eventually to where companies outsourced entire portions of their infrastructure (think Hosted Exchange, Hosted CRM, Hosted OCS).",
    "categories": ["Cloud Computing"],
    "tags": ["cloud"]
  },
  "2009-bju-programming-contest": {
    "layout": "../_tech_post",
    "date": "March 24, 2009",
    "title": "2009 BJU Programming Contest",
    "excerpt": "I had the privilege of being one of the alumni-judges at the annual Bob Jones University Computer Science departments programming contest. This was the first time I’ve participated in this type of contest and I found it very interesting. The CS department had a fairly slick harness for executing the contest and supporting the judging in multiple languages and multiple platforms. As with anything of this nature, there were a few bumps in the road, but nothing of any consequence",
    "categories": ["General Development"],
    "tags": ["personal"]
  },
  "misplaced-modifier": {
    "layout": "../_tech_post",
    "date": "March 13, 2009",
    "title": "Misplaced Modifier?",
    "excerpt": "I was on a few websites this morning (go figure) and noticed a handful of visual missteps (mostly minor). One of them was on the Mix 09 conference page and made me chuckle a little.",
    "categories": ["Miscellaneous"],
    "tags": ["fun", "mix"]
  },
  "index" : {
    "layout": "index"
  }
}

