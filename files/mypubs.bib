
@misc{haas_doe_2018,
	title = {{DOE} {CODE}: {Project} {Metadata} for {Code} {ID} 45620},
	url = {https://www.osti.gov/doecode/biblio/45620},
	abstract = {MathWorks' MATLAB is widely used in academia and industry for prototyping, data analysis, data processing, etc. Many users compile their programs using the MATLAB Compiler to run on workstations/computing clusters via the free MATLAB Compiler Runtime (MCR). The MCR facilitates the execution of code calling Application Programming Interfaces (API) functions from both base MATLAB and MATLAB toolboxes. In a Linux environment, a sizable number of third-party runtime dependencies (i.e. shared libraries) are necessary. Unfortunately, to the MTLAB community's knowledge, these dependencies are not documented, leaving system administrators and/or end-users to find/install the necessary libraries either as runtime errors resulting from them missing or by inspecting the header information of Executable and Linkable Format (ELF) libraries of the MCR to determine which ones are missing from the system. To address various shortcomings, Docker Images based on Community Enterprise Operating System (CentOS) 7, a derivative of Redhat Enterprise Linux (RHEL) 7, containing recent (2015-2017) MCR releases and their dependencies were created. These images, along with a provided sample Docker Compose YAML Script, can be used to create a simulated computing cluster where MATLAB Compiler created binaries can be executed using a sample Slurm Workload Manager script.},
	urldate = {2020-10-14},
	author = {Haas, Nicholas Q. and Gillen, Robert E. and Karnowski, Thomas P.},
	month = jan,
	year = {2018}
}

@misc{oesch_doe_2018,
	title = {{DOE} {CODE}: {Project} {Metadata} for {Code} {ID} 45685},
	url = {https://www.osti.gov/doecode/biblio/45685},
	abstract = {ShareAnalytics is a lightweight extensible platform for collaborative data analytics and facility characterization via Non-intrusive Load Monitoring (NILM). It consists of three core components: a full stack web application, a dashboard tool for analyzing streaming data and a High Performance Computing (HPC) cluster leveraging slurm for resource management to perform real time analysis. The entire framework is implemented in Docker for ease of deployment and to allow the user to test the platform before making a significant investment in hardware if they require additional computing resources. While many other platforms for analyzing streaming data exists, ShareAnalytics is unique in that HPC capabilities and facility characterization via scored analytics are fully integrated into the platform.},
	urldate = {2020-10-14},
	author = {Oesch, Timothy S. and Gillen, Robert E. and Haas, Nicholas Q. and Karnowski, Thomas P.},
	month = jul,
	year = {2018}
}

@inproceedings{logasa_bogen_massively_2013,
	title = {Massively scalable near duplicate detection in streams of documents using {MDSH}},
	doi = {10.1109/BigData.2013.6691610},
	abstract = {In a world where large-scale text collections are not only becoming ubiquitous but also are growing at increasing rates, near duplicate documents are becoming a growing concern that has the potential to hinder many different information filtering tasks. While others have tried to address this problem, prior techniques have only been used on limited collection sizes and static cases. We will briefly describe the problem in the context of Open Source analysis along with our additional constraints for performance. In this work we propose two variations on Multi-dimensional Spectral Hash (MDSH) tailored for working on extremely large, growing sets of text documents. We analyze the memory and runtime characteristics of our techniques and provide an informal analysis of the quality of the near-duplicate clusters produced by our techniques.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Big} {Data}},
	author = {Logasa Bogen, Paul and Symons, Christopher T. and McKenzie, Amber and Patton, Robert M. and Gillen, Robert E.},
	month = oct,
	year = {2013},
	keywords = {Big Data, Electronic publishing, Encyclopedias, Internet, MDSH, Memory management, Near Duplicate Detection, Open Source Intelligence, Random access memory, Runtime, Streaming Text, document stream, file organisation, information filtering, information filtering task, large-scale text collections, memory characteristics, multidimensional spectral hash, near duplicate detection, near duplicate documents, near-duplicate clusters, open source analysis, public domain software, quality informal analysis, runtime characteristics, text analysis, text documents},
	pages = {480--486}
}

@inproceedings{mckenzie_redeye_2013,
	title = {Redeye {Text} {Analysis} {Workbench}: {Actionable} intelligence from non-actionable data},
	shorttitle = {Redeye {Text} {Analysis} {Workbench}},
	doi = {10.1109/THS.2013.6699034},
	abstract = {With the increase in digital data within the realm of law enforcement digital forensics comes significant challenges in the form of data discovery and analysis. The Redeye Text Analysis Workbench, produced by Oak Ridge National Laboratory, seeks to bridge the gap between existing data acquisition and higher-level data analysis systems in which forensic analysts must currently manually identify pertinent and relevant documents from possibly many terabytes of data that can then be used as input for further examination. The Redeye toolkit comprises two separate components: an ingestion pipeline and the workbench interface. Both of these components incorporate a number of tools - open source, proprietary, and custom-built - which have been integrated together to transcend the tools themselves. The final application is a means to facilitate the discovery of interesting, useful data within a digital forensics investigation.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Technologies} for {Homeland} {Security} ({HST})},
	author = {McKenzie, Amber and Gillen, Rob and Logasa Bogen, Paul},
	month = nov,
	year = {2013},
	keywords = {Databases, Digital forensics, Law enforcement, Libraries, Oak Ridge National Laboratory, Pipelines, Redeye toolkit, Text analysis, actionable intelligence, data acquisition, data analysis, data discovery, digital data, digital forensics, digital forensics investigation, forensic analysts, ingestion pipeline, law enforcement digital forensics, legislation, nonactionable data, redeye, redeye text analysis workbench, relevant documents, text analysis, workbench interface},
	pages = {381--385}
}

@inproceedings{gillen_design_2020,
	address = {Cork, Ireland},
	title = {Design and {Implementation} of {Full}-{Scale} {Industrial} {Control} {System} {Test} {Bed} for {Assessing} {Cyber}-{Security} {Defenses}},
	isbn = {9781728173740},
	url = {https://ieeexplore.ieee.org/document/9217693/},
	doi = {10.1109/WoWMoM49955.2020.00064},
	abstract = {In response to the increasing awareness of the Ethernet-based threat surface of industrial control systems (ICS), both the research and commercial communities are responding with ICS-specific security solutions. Unfortunately, many of the properties of ICS environments that contribute to the extent of this threat surface (e.g. age of devices, inability or unwillingness to patch, criticality of the system) similarly prevent the proper testing and evaluation of these security solutions. Production environments are often too fragile to introduce unvetted technology and most organizations lack test environments that are sufficiently consistent with production to yield actionable results. Cost and space requirements prevent the creation of mirrored physical environments leading many to look towards simulation or virtualization. Examples in literature provide various approaches to building ICS test beds, though most of these suffer from a lack of realism due to contrived scenarios, synthetic data and other compromises. In this paper, we provide a design methodology for building highly realistic ICS test beds for validating cybersecurity defenses. We then apply that methodology to the design and building of a specific test bed and describe the results and experimental use cases.},
	urldate = {2020-10-14},
	booktitle = {2020 {IEEE} 21st {International} {Symposium} on "{A} {World} of {Wireless}, {Mobile} and {Multimedia} {Networks}" ({WoWMoM})},
	publisher = {IEEE},
	author = {Gillen, Robert E. and Anderson, Laura Ann and Craig, Christopher and Johnson, Jordan and Columbia, Adam and Anderson, Rachel and Craig, Andrew and Scott, Stephen L.},
	month = aug,
	year = {2020},
	pages = {341--346}
}

@inproceedings{gillen_assessing_2020,
	address = {Cork, Ireland},
	title = {Assessing {Anomaly}-{Based} {Intrusion} {Detection} {Configurations} for {Industrial} {Control} {Systems}},
	isbn = {9781728173740},
	url = {https://ieeexplore.ieee.org/document/9217655/},
	doi = {10.1109/WoWMoM49955.2020.00067},
	abstract = {To reduce cost and ease maintenance, industrial control systems (ICS) have adopted Ethernet based interconnections that integrate operational technology (OT) systems with information technology (IT) networks. This integration has made these critical systems vulnerable to attack. Security solutions tailored to ICS environments are an active area of research. Anomaly based network intrusion detection systems are well-suited for these environments. Often these systems must be optimized for their specific environment. In prior work, we introduced a method for assessing the impact of various anomaly-based network IDS settings on security. This paper reviews the experimental outcomes when we applied our method to a full-scale ICS test bed using actual attacks. Our method provides new and valuable data to operators enabling more informed decisions about IDS configurations.},
	urldate = {2020-10-14},
	booktitle = {2020 {IEEE} 21st {International} {Symposium} on "{A} {World} of {Wireless}, {Mobile} and {Multimedia} {Networks}" ({WoWMoM})},
	publisher = {IEEE},
	author = {Gillen, Robert E. and Carter, Jason M. and Craig, Christopher and Johnson, Jordan A. and Scott, Stephen L.},
	month = aug,
	year = {2020},
	pages = {360--366}
}

@inproceedings{bogen_redeye_2013,
	address = {Indianapolis, Indiana, USA},
	series = {{JCDL} '13},
	title = {Redeye: a digital library for forensic document triage},
	isbn = {9781450320771},
	shorttitle = {Redeye},
	url = {https://doi.org/10.1145/2467696.2467716},
	doi = {10.1145/2467696.2467716},
	abstract = {Forensic document analysis has become an important aspect of investigation of many different kinds of crimes from money laundering to fraud and from cybercrime to smuggling. The current workflow for analysts includes powerful tools, such as Palantir and Analyst's Notebook, for moving from evidence to actionable intelligence and tools for finding documents among the millions of files on a hard disk, such as Forensic Toolkit (FTK). Analysts often leave the process of sorting through collections of seized documents to filter out noise from actual evidence to highly labor-intensive manual efforts. This paper presents the Redeye Analysis Workbench, a tool to help analysts move from manual sorting of a collection of documents to performing intelligent document triage over a digital library. We will discuss the tools and techniques we build upon in addition to an in-depth discussion of our tool and how it addresses two major use cases we observed analysts performing. Finally, we also include a new layout algorithm for radial graphs that is used to visualize clusters of documents in our system.},
	urldate = {2020-10-14},
	booktitle = {Proceedings of the 13th {ACM}/{IEEE}-{CS} joint conference on {Digital} libraries},
	publisher = {Association for Computing Machinery},
	author = {Bogen, Paul L. and McKenzie, Amber and Gillen, Rob},
	month = jul,
	year = {2013},
	keywords = {document triage, forensic science, redeye},
	pages = {181--190}
}

@patent{gillen_cloud_2012,
	title = {Cloud computing method for dynamically scaling a process across physical machine boundaries},
	url = {https://patents.google.com/patent/US8825710B2/en},
	abstract = {A cloud computing platform includes first device having a graph or tree structure with a node which receives data. The data is processed by the node or communicated to a child node for processing. A first node in the graph or tree structure determines the reconfiguration of a portion of the graph or tree structure on a second device. The reconfiguration may include moving a second node and some or all of its descendant nodes. The second and descendant nodes may be copied to the second device.},
	language = {en},
	urldate = {2020-10-14},
	author = {Gillen, Robert E. and Patton, Robert M. and Potok, Thomas E. and Rojas, Carlos C.},
	month = may,
	year = {2012}
}

@patent{symons_-situ_2014,
	title = {In-situ trainable intrusion detection system},
	url = {https://patents.google.com/patent/US9497204B2/en},
	abstract = {A computer implemented method detects intrusions using a computer by analyzing network traffic. The method includes a semi-supervised learning module connected to a network node. The learning module uses labeled and unlabeled data to train a semi-supervised machine learning sensor. The method records events that include a feature set made up of unauthorized intrusions and benign computer requests. The method identifies at least some of the benign computer requests that occur during the recording of the events while treating the remainder of the data as unlabeled. The method trains the semi-supervised learning module at the network node in-situ, such that the semi-supervised learning modules may identify malicious traffic without relying on specific rules, signatures, or anomaly detection.},
	language = {en},
	urldate = {2020-10-14},
	author = {Symons, Christopher T. and Beaver, Justin M. and Gillen, Robert E. and Potok, Thomas E.},
	month = aug,
	year = {2014}
}

@inproceedings{beaver_learning_2013,
	address = {Oak Ridge, Tennessee, USA},
	series = {{CSIIRW} '13},
	title = {A learning system for discriminating variants of malicious network traffic},
	isbn = {9781450316873},
	url = {https://doi.org/10.1145/2459976.2460003},
	doi = {10.1145/2459976.2460003},
	abstract = {Modern computer network defense systems rely primarily on signature-based intrusion detection tools, which generate alerts when patterns that are pre-determined to be malicious are encountered in network data streams. Signatures are created reactively, and only after in-depth manual analysis of a network intrusion. There is little ability for signature-based detectors to identify intrusions that are new or even variants of an existing attack, and little ability to adapt the detectors to the patterns unique to a network environment. Due to these limitations, the need exists for network intrusion detection techniques that can more comprehensively address both known and unknown network-based attacks and can be optimized for the target environment. This work describes a system that leverages machine learning to provide a network intrusion detection capability that analyzes behaviors in channels of communication between individual computers. Using examples of malicious and non-malicious traffic in the target environment, the system can be trained to discriminate between traffic types. The machine learning provides insight that would be difficult for a human to explicitly code as a signature because it evaluates many interdependent metrics simultaneously. With this approach, zero day detection is possible by focusing on similarity to known traffic types rather than mining for specific bit patterns or conditions. This also reduces the burden on organizations to account for all possible attack variant combinations through signatures. The approach is presented along with results from a third-party evaluation of its performance.},
	urldate = {2020-10-14},
	booktitle = {Proceedings of the {Eighth} {Annual} {Cyber} {Security} and {Information} {Intelligence} {Research} {Workshop}},
	publisher = {Association for Computing Machinery},
	author = {Beaver, Justin M. and Symons, Christopher T. and Gillen, Robert E.},
	month = jan,
	year = {2013},
	keywords = {computer network defense, intrusion detection, machine learning},
	pages = {1--4}
}